{"created":"2023-10-15T23:53:10.611641Z","extra":{"kind":"study","sourceInputsCount":0},"lastVersionMaj":"2.0","scriptAccess":"open_no_auth","scriptName":"Neural Network Showcase | Alien_Algorithms","source":"// This work is licensed under a Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) https://creativecommons.org/licenses/by-nc-sa/4.0/\n//@version=5\nindicator(\"Neural Network Showcase | Alien_Algorithms\", overlay=true, shorttitle=\"Neural Showcase | Alien_Algorithms\")\n\nlr = input.float(title='Learning Rate', defval=0.1, minval=0.00001)\nepochs = input.int(title='Epochs', defval=60, minval=10, maxval=1000)\nuse_simple_backprop = input(defval=false, title='Simple Backpropagation')\n\nplot_loss_curve = input(true, 'Plot Loss Curve', group='Statistics')\nchart_scaling = input.int(title='Chart Scaling Factor', defval=1, minval=1, group = 'Statistics')\nhorizontal_offset = input.int(title='Chart Horizontal Offset', defval = 100, group='Statistics')\nvertical_offset = input.int(title='Chart Vertical Offset (percentage)', defval = -2, group='Statistics') \nvertical_offset_pct = 1 + (vertical_offset / 100)\n\n// Begin logging the maximum price, to be used in normalization\nvar max_scale = 0.0\nmax_scale := high > max_scale ? high : max_scale\n\n// Initialize weight matrices at random for better feature distribution\nvar w1 = matrix.new<float>(2, 2, 0.0)\nvar w2 = matrix.new<float>(1, 2, 0.0)\n\n\n// Function to fill each element of a matrix with random values, while maintaining reproducibility with a seed. \n// This is needed because matrix.new() can only set the same value for the entire matrix. \nfillMatrixRandomly(matrix, rows, cols) =>\n    seed = 1337  // Any fixed number as a seed will do\n    for i = 0 to rows - 1\n        for j = 0 to cols - 1\n            // The seed is altered for each matrix element to ensure different values\n            // while still being repeatable on every run\n            matrix.set(matrix, i, j, math.random(0, 1, seed + i + j * rows))\n\n// Fill w1 and w2 with random values\n// It is important that the weights are not initialized with the same values, to induce asymmetry during gradient updates.\n// This allows the network to utilize all the weights effectively\nif barstate.isfirst\n    fillMatrixRandomly(w1, 2, 2)\n    fillMatrixRandomly(w2, 1, 2)\n\n// Sigmoid activation function\nsigmoid(x) =>\n    1 / (1 + math.exp(-x))\n\n// Mean Squared Error Loss function\nmse_loss(predicted, actual) =>\n    math.pow(predicted - actual, 2)\n\n// Normalize the data between 0 and 1\nnormalize(data) =>\n    data / max_scale\n\n// Revert the data back to the original form\nstandardize(data) =>\n    data * max_scale\n\n// Feed forward through the neural network\n// This process passes the input data through the network, and obtains an output\nfeedforward(input) =>\n    hidden_out = array.new_float(0)\n\n    // Push through the first layer\n    for i = 0 to 1\n        sum = 0.0\n        for j = 0 to 1\n            sum := sum + w1.get(i, j) * array.get(input, j)\n        array.push(hidden_out, sigmoid(sum))\n\n    // Push through the second layer\n    output = 0.0\n    for i = 0 to 1\n        output := output + w2.get(0, i) * array.get(hidden_out, i)\n    output := sigmoid(output)\n    [output, hidden_out]\n\n// Backpropagation observes the difference in actual and predicted values (obtained from the feedforward), and adjusts the weights using a gradient to lower the error (loss).\nbackpropagation_simple(input, actual_output, predicted_output, hidden_out) => \n\n    // Update weights of the second layer\n    for i = 0 to 1\n        w2.set(0, i, w2.get(0, i) - lr * 2 * (predicted_output - actual_output) * array.get(hidden_out, i))\n\n    // Update weights of the first layer\n    for i = 0 to 1\n        for j = 0 to 1\n            w1.set(i, j, w1.get(i, j) - lr * 2 * (predicted_output - actual_output) * w2.get(0, i) * array.get(input, j))\n\nbackpropagation_verbose(input, actual_output, predicted_output, hidden_out) =>\n    // Calculate the derivative of the loss with respect to the output (MSE loss)\n    float d_loss_d_output = 2 * (predicted_output - actual_output)\n    \n    // Update weights of the second layer\n    for i = 0 to 1\n        float hidden_val = array.get(hidden_out, i)\n        float d_loss_d_w2 = d_loss_d_output * hidden_val\n        w2.set(0, i, w2.get(0, i) - lr * d_loss_d_w2)\n    \n    // Update weights of the first layer\n    for i = 0 to 1\n        for j = 0 to 1\n            float input_val = array.get(input, j)\n            float w2_val = w2.get(0, i)\n            float hidden_val = array.get(hidden_out, i)\n            float sigmoid_derivative = hidden_val * (1 - hidden_val)\n            float d_loss_d_w1 = d_loss_d_output * w2_val * sigmoid_derivative * input_val\n            w1.set(i, j, w1.get(i, j) - lr * d_loss_d_w1)\n\n\n// A wrapper function that trains the neural network with the set parameters (Learning Rate, Epochs)\ntrain_nn(input, actual_output) =>\n    loss_curve = array.new<float>(0)\n\n    for epoch = 1 to epochs \n        // Predicting\n        [predicted_output, hidden_out] = feedforward(input)\n        loss = mse_loss(predicted_output, actual_output)\n        \n        // Metrics\n        log.warning(\"~~~~ Epoch {0} ~~~~\", epoch)\n        log.info(\"Loss: {0}\", str.tostring(loss))\n        array.push(loss_curve, loss)\n\n        // Weight Adjustment (Training)\n        if use_simple_backprop\n            backpropagation_simple(input, actual_output, predicted_output, hidden_out)\n        else\n            backpropagation_verbose(input, actual_output, predicted_output, hidden_out)\n    \n    loss_curve\n\n\n\n// Define input and output variables that the network will use.\nfloat[] input = array.new_float(0)\narray.push(input, normalize(close[1]))\narray.push(input, normalize(close[2]))\nactual_output = normalize(close)\n\n// Perform training only on the last confirmed bar to save resources\nfloat predicted_output = na\nif barstate.islastconfirmedhistory\n    // Training updates all the weights and returns a loss curve containing the loss for each epoch in an array, which can then be visualized.\n    loss_curve = train_nn(input, actual_output)\n\n    // Get neural network output\n    [predicted_output_normalized, _] = feedforward(input)\n    predicted_output := standardize(predicted_output_normalized)\n\n\n    log.error(str.tostring(w1))\n    log.error(str.tostring(w2))\n    // ~~~~~~~~~~~~~ Plot the neural network output ~~~~~~~~~~~~~\n    label.new(bar_index+40, predicted_output, text = 'Predicted Output', style = label.style_label_lower_left, color=color.purple, textcolor = color.white, tooltip = 'The price which the neural network predicted')\n    line.new(bar_index, predicted_output, bar_index+40, predicted_output, color=color.purple, width=4)\n\n    label.new(bar_index+40, standardize(actual_output), text = 'Actual Output',style = label.style_label_lower_left, color=color.green, textcolor = color.black, tooltip = 'The price which the neural network aims to predict')\n    line.new(bar_index, standardize(actual_output), bar_index+40, standardize(actual_output), color=color.green, width=4)\n\n    mid = math.abs(standardize(actual_output) - predicted_output)  / 2\n    mid := predicted_output > standardize(actual_output) ? predicted_output - mid : predicted_output + mid\n\n    label.new(bar_index+10, mid, text = 'MSE Loss: '+str.tostring(array.get(loss_curve,epochs-1)), color=color.rgb(195, 195, 195), style=label.style_label_left, textcolor = color.black, tooltip = 'The rate of error between the prediction and actual value')\n    line.new(bar_index+10, predicted_output, bar_index+10, standardize(actual_output), color=color.rgb(177, 177, 177), style=line.style_dashed, width=1)\n    \n    if plot_loss_curve\n        size = array.size(loss_curve)\n        float max_loss = array.max(loss_curve)\n        float min_loss = array.min(loss_curve)\n        float loss_range = max_loss - min_loss\n\n        float chart_range = (high - low) / chart_scaling\n        float scaling_factor = chart_range / loss_range\n\n        var points = array.new<chart.point>()\n        for i = 0 to size - 1\n            float normalized_loss = (array.get(loss_curve, i) - min_loss) / loss_range  // Normalize to [0, 1]\n            float scaled_loss = normalized_loss * scaling_factor  // Scale to chart range\n            float shifted_loss = scaled_loss + (close * vertical_offset_pct)  // Shift to match chart values\n            point = chart.point.new(time+i+horizontal_offset, bar_index+i+horizontal_offset, shifted_loss)\n            points.push(point)\n\n        float first_loss = (array.get(loss_curve, 0) - min_loss) / loss_range * scaling_factor + (close * vertical_offset_pct)\n        float last_loss = (array.get(loss_curve, size-1) - min_loss) / loss_range * scaling_factor + (close * vertical_offset_pct)\n\n        label.new(bar_index+horizontal_offset+size, last_loss - 0.01 * scaling_factor, text = 'Loss Curve', style = label.style_label_upper_left, color=color.rgb(87, 87, 87), textcolor = color.rgb(255, 255, 255), tooltip='The MSE Loss (y-axis) plotted over the epoch iterations (x-axis)')\n        box.new(bar_index+horizontal_offset, first_loss, bar_index+horizontal_offset+size, last_loss - 0.01 * scaling_factor, bgcolor = color.rgb(120, 123, 134, 81),border_width = 3)\n        polyline.new(points, curved=true, line_color=color.rgb(194, 208, 0), line_width = 1)\n","updated":"2023-10-15T23:53:10.611641Z","version":"2.0"}